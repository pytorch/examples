{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-level Language Modeling and the torchtext Package\n",
    "======================================================\n",
    "\n",
    "The PyTorch [torchtext package](https://github.com/pytorch/text)\n",
    "consists of data processing utilities and popular\n",
    "datasets for natural language. Its goals are to ease the painful and tedious\n",
    "efforts of preprocessing text data for Natural Language Processing (NLP).\n",
    "\n",
    "The steps below will demonstrate a use case of using `torchtext` in place of the\n",
    "data preprocessing code used in the PyTorch\n",
    "\"Word-level Language Modeling\"\n",
    "[example](https://github.com/pytorch/examples/blob/master/word_language_model/main.py).\n",
    "The aim is to keep the original code as much as possible, while at the same time,\n",
    "showcase various features of torchtext to perform data loading, tokenization,\n",
    "numericalization, vocabulary building, using pre-trained word embeddings, and\n",
    "batching of processed data for model consumption.\n",
    "\n",
    "Official `torchtext` documentation can be found\n",
    "[here](http://torchtext.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "To begin, please make sure that `torchtext` is installed using the instructions provided\n",
    "in the [torchtext package](https://github.com/pytorch/text). Please also install the\n",
    "[SpaCy](https://spacy.io/) package since it will be used for tokenization in this demo (`pip install spacy`).\n",
    "In addition, this notebook will be run with the \"Word-level Language Modeling\"\n",
    "[example](https://github.com/pytorch/examples/blob/master/word_language_model)\n",
    "environment as the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
    "\n",
    "# Setting --wordem or --emsize as mutually exclusive flags\n",
    "group = parser.add_mutually_exclusive_group(required=True)\n",
    "group.add_argument('--wordem', type=str,\n",
    "                    help='name of word embeddings. For example:\"glove.6B.200d\". \\\n",
    "                    For aliases names, see  \\\n",
    "                    http://torchtext.readthedocs.io/en/latest/vocab.html#vectors')\n",
    "group.add_argument('--emsize', type=int,\n",
    "                    help='size of word embeddings')\n",
    "\n",
    "#parser.add_argument('--data', type=str, default='./data/wikitext-2',\n",
    "#                    help='location of the data corpus')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--onnx-export', type=str, default='',\n",
    "                    help='path to export the final model in onnx format')\n",
    "\n",
    "# Since '--embsize' and '--wordem' are mutually exclusive required options,\n",
    "# one of the two options must be provided. The following example we will\n",
    "# run training on GPU for 1 epoch with embedding size equals to 200\n",
    "args = parser.parse_args(['--epochs', '1', '--cuda', '--emsize','200'])\n",
    "\n",
    "# Example of setting the wordem (word embeddings) flags.\n",
    "# See http://torchtext.readthedocs.io/en/latest/vocab.html#vectors for\n",
    "# pre-trained word embeddings alias names.\n",
    "# args = parser.parse_args(['--epochs', '1', '--cuda', '--wordem', 'glove.6B.200d'])\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torchtext to process data\n",
    "\n",
    "The following are the steps involved to preprocess input text data for model\n",
    "consumption:\n",
    "\n",
    "* Define [Fields](http://torchtext.readthedocs.io/en/latest/data.html#fields) object(s)\n",
    "     \n",
    "  The Field object(s) defines a datatype together with instructions for\n",
    "  converting input to Tensor. For example: Should the data be numericalized or\n",
    "  not?\n",
    "     \n",
    "  \n",
    "* Create [Dataset](http://torchtext.readthedocs.io/en/latest/data.html#dataset) object(s)\n",
    "  \n",
    "  Create Dataset object(s) from input text data by processing text data per the\n",
    "  definition defined by the Fields object(s).\n",
    "  \n",
    "  \n",
    "* Create [Vocab](http://torchtext.readthedocs.io/en/latest/vocab.html) object(s)\n",
    "  \n",
    "  Create vocabulary object(s) which contain the vocabularies from the input\n",
    "  datasets. Initialize the objects(s) with pre-trained word embeddings if\n",
    "  specified. \n",
    "  \n",
    "  \n",
    "* Define [Iterators](http://torchtext.readthedocs.io/en/latest/data.html#iterators)\n",
    "  \n",
    "  Define iterator(s) to load batches of data from datasets.\n",
    "  \n",
    "\n",
    "### Define Field object\n",
    "In this demo, SpaCy will be used as the tokenizer.  This is not a hard\n",
    "requirement, as any other tokenizer should work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torchtext to create train, validation and test datasets\n",
    "\n",
    "# 1. create Field objects using SpaCy as the tokenizer. SpaCy is not a hard\n",
    "#    requirement, as any other tokenizer should work too.\n",
    "TEXT = torchtext.data.Field(tokenize=torchtext.data.get_tokenizer('spacy'),\n",
    "                 eos_token='<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets\n",
    "\n",
    "The `torchtext` module provides built-in Dataset classes for a number of\n",
    "commonly used datasets such as wikiText-2 for language modeling, SST and IMDb\n",
    "for sentiment analysis, etc.  In ths demonstration, we will use the wikiText-2\n",
    "Dataset class and the TEXT field object defined above to create datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train dataset:      2236652\n",
      "Length of the validation dataset: 245042\n",
      "Length of the test dataset:       280576\n"
     ]
    }
   ],
   "source": [
    "# 2. create train, validation and test datasets by processing the input data\n",
    "#    per the definition defined by the \"TEXT\" (Field) object.\n",
    "train_ds,val_ds,test_ds = torchtext.datasets.WikiText2.splits(text_field=TEXT)\n",
    "\n",
    "print(\"Length of the train dataset:     \", len(train_ds.examples[0].text))\n",
    "print(\"Length of the validation dataset:\", len(val_ds.examples[0].text))\n",
    "print(\"Length of the test dataset:      \", len(test_ds.examples[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab object\n",
    "\n",
    "Build the Vocab object from the *train* dataset and initialize it with the\n",
    "pre-trained word embeddings if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vocabularies in the dataset: 33244\n"
     ]
    }
   ],
   "source": [
    "# 3. create Vocab object\n",
    "if args.wordem:\n",
    "    TEXT.build_vocab(train_ds, vectors = args.wordem)\n",
    "    args.emsize = TEXT.vocab.vectors.size(1)\n",
    "else:\n",
    "    TEXT.build_vocab(train_ds)\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "print(\"Number of unique vocabularies in the dataset:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Iterators\n",
    "\n",
    "Create iterators to load batches of data from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create iterators\n",
    "train_iter = torchtext.data.BPTTIterator(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    bptt_len=args.bptt,\n",
    "    device=(0 if args.cuda else -1),\n",
    "    repeat=False)\n",
    "\n",
    "eval_batch_size = 10\n",
    "val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (val_ds, test_ds),\n",
    "    batch_size=eval_batch_size,\n",
    "    bptt_len=args.bptt,\n",
    "    device=(0 if args.cuda else -1),\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensor, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def evaluate(model, criterion, ds, ds_iter, ntokens, batch_size):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(ds_iter):\n",
    "            text, targets = batch.text, batch.target.view(-1)\n",
    "            output, hidden = model(text, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(text) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(ds.examples[0].text)\n",
    "\n",
    "\n",
    "def train(model, criterion, ds_iter, ntokens, epoch, args):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch_num, batch  in enumerate(ds_iter):\n",
    "        text, targets = batch.text, batch.target.view(-1)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-args.lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_num % args.log_interval == 0 and batch_num > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch_num, len(ds_iter), args.lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time Mon Jul 23 10:11:23 2018\n",
      "| epoch   1 |   200/ 3196 batches | lr 20.00 | ms/batch 72.13 | loss  7.20 | ppl  1339.30\n",
      "| epoch   1 |   400/ 3196 batches | lr 20.00 | ms/batch 69.55 | loss  6.33 | ppl   559.32\n",
      "| epoch   1 |   600/ 3196 batches | lr 20.00 | ms/batch 69.55 | loss  6.01 | ppl   407.06\n",
      "| epoch   1 |   800/ 3196 batches | lr 20.00 | ms/batch 69.45 | loss  5.80 | ppl   331.70\n",
      "| epoch   1 |  1000/ 3196 batches | lr 20.00 | ms/batch 69.41 | loss  5.79 | ppl   327.89\n",
      "| epoch   1 |  1200/ 3196 batches | lr 20.00 | ms/batch 69.38 | loss  5.70 | ppl   299.46\n",
      "| epoch   1 |  1400/ 3196 batches | lr 20.00 | ms/batch 69.46 | loss  5.59 | ppl   268.33\n",
      "| epoch   1 |  1600/ 3196 batches | lr 20.00 | ms/batch 69.45 | loss  5.47 | ppl   238.39\n",
      "| epoch   1 |  1800/ 3196 batches | lr 20.00 | ms/batch 69.40 | loss  5.49 | ppl   241.93\n",
      "| epoch   1 |  2000/ 3196 batches | lr 20.00 | ms/batch 69.41 | loss  5.47 | ppl   237.07\n",
      "| epoch   1 |  2200/ 3196 batches | lr 20.00 | ms/batch 69.39 | loss  5.38 | ppl   217.83\n",
      "| epoch   1 |  2400/ 3196 batches | lr 20.00 | ms/batch 69.43 | loss  5.25 | ppl   191.18\n",
      "| epoch   1 |  2600/ 3196 batches | lr 20.00 | ms/batch 69.47 | loss  5.32 | ppl   204.02\n",
      "| epoch   1 |  2800/ 3196 batches | lr 20.00 | ms/batch 69.39 | loss  5.28 | ppl   196.99\n",
      "| epoch   1 |  3000/ 3196 batches | lr 20.00 | ms/batch 69.36 | loss  5.16 | ppl   174.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 230.78s | valid loss  0.49 | valid ppl     1.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Training end time Mon Jul 23 10:15:14 2018\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Training with RNNModel\n",
    "# ========================================================\n",
    "ntokens = len(vocab)\n",
    "best_val_loss = None\n",
    "lr = args.lr\n",
    "\n",
    "# Create model\n",
    "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid,\n",
    "                       args.nlayers, args.dropout, args.tied).to(device)\n",
    "if args.wordem:\n",
    "    print(\"Initialize word embeddings vectors\")\n",
    "    model.encoder.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training start time\", time.asctime( time.localtime(time.time())))\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, criterion, train_iter, ntokens, epoch, args )\n",
    "    val_loss = evaluate(model, criterion, val_ds, val_iter, ntokens, eval_batch_size)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(args.save, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "\n",
    "print(\"Training end time\", time.asctime( time.localtime(time.time())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
